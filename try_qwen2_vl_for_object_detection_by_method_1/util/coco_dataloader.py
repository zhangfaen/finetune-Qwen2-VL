import sys
from pathlib import Path
sys.path.append(Path(__file__).resolve().parents[2].as_posix()+"/util")  # TODO: fix this ugly import hack

import torch
from datasets import load_dataset
from torch.utils.data import DataLoader
from functools import partial
from vision_util import process_vision_info

# print(processor.tokenizer.vocab_size)
# print(processor.tokenizer.additional_special_tokens_ids)
# print(processor.tokenizer.decode([151656]))
# print(processor.tokenizer.decode([151657])) # return None
# print(processor.tokenizer.decode([151640, 151641, 151642, 151643, 151644, 151645, 151646, 151647, 151648, 151649, 151650, 151651, 151652, 151653, 151654, 151655, 151656]))

# 151643
# [151644, 151645, 151646, 151647, 151648, 151649, 151650, 151651, 151652, 151653, 151654, 151655, 151656]
# <|video_pad|>

# â¨âºŸâ½—<|endoftext|><|im_start|><|im_end|><|object_ref_start|><|object_ref_end|><|box_start|><|box_end|><|quad_start|><|quad_end|><|vision_start|><|vision_end|><|vision_pad|><|image_pad|><|video_pad|>

dataset = load_dataset("rafaelpadilla/coco2017")

train_dataset = dataset["train"]
val_dataset = dataset["val"]
labels = ["None", "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light", "fire hydrant", "street sign", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "hat", "backpack", "umbrella", "shoe", "eye glasses", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", "plate", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", "mirror", "dining table", "window", "desk", "toilet", "door", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator", "blender", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush", "hair brush"]

BINS_FOR_LOC_SIZE = 100
BINS_FOR_LOC_RANGE = (151642-BINS_FOR_LOC_SIZE, 151642) # We use the least used 1000 bins for location. **Note**  151643 is <|endoftext|>
BINS_FOR_OBJ_LABEL_SIZE = len(labels)
BINS_FOR_OBJ_LABEL_RANGE = (151642-BINS_FOR_LOC_SIZE-len(labels), 151642-BINS_FOR_LOC_SIZE) # We use the least used len(labels) bins for object label.

def split_into_complete_groups_of_five(lst):
    groups = [lst[i:i + 5] for i in range(0, len(lst), 5)]
    # Remove the last group if it's not complete
    if len(groups) > 0 and len(groups[-1]) != 5:
        groups.pop()
    return groups

def convert_token_ids_to_bbox_xyhw_and_label_id(token_ids, processor, image_size):
    '''
    We have a list of tokens, and we want to convert it back to bbox and label.
    The list of tokens and token ids look like:
    tokens: <obj_label_token><loc_token><loc_token><loc_token><loc_token><obj_label_token><loc_token><loc_token><loc_token><loc_token>...
    '''

    bbox = []
    label_id = []
    for group in split_into_complete_groups_of_five(token_ids):
        one_box = []
        if group[0] not in range(*BINS_FOR_OBJ_LABEL_RANGE):
            print(f"Invalid object label token: {group[0]}")
            return [],[]
        for loc_id in group[1:]:
            if loc_id not in range(*BINS_FOR_LOC_RANGE):
                print(f"Invalid loc token: {loc_id}")
                return [],[]
        
        label_id.append(group[0]-BINS_FOR_OBJ_LABEL_RANGE[0])
        
        xmin = (group[1]-BINS_FOR_LOC_RANGE[0])*image_size[0]/BINS_FOR_LOC_SIZE
        one_box.append(xmin)

        ymin = (group[2]-BINS_FOR_LOC_RANGE[0])*image_size[1]/BINS_FOR_LOC_SIZE
        one_box.append(ymin)
     
        xmax = (group[3]-BINS_FOR_LOC_RANGE[0])*image_size[0]/BINS_FOR_LOC_SIZE
        one_box.append(xmax-xmin)
      
        ymax = (group[4]-BINS_FOR_LOC_RANGE[0])*image_size[1]/BINS_FOR_LOC_SIZE
        one_box.append(ymax-ymin)

        bbox.append(one_box)
    
    return bbox, label_id

def convert_bbox_xyhw_and_label_id_to_tokens(bbox, label_id, processor, image_size):
    '''
    (Pdb++) bbox
    [[265.05999755859375, 126.02999877929688, 33.86000061035156, 66.26000213623047], [20.600000381469727, 1.0700000524520874, 270.4100036621094, 382.42999267578125], [268.6099853515625, 69.66000366210938, 222.67999267578125, 88.9000015258789]]
    (Pdb++) object_label
    [90, 1, 81]
    (Pdb++) image_size
    (640, 573) 
    '''
    assert(len(bbox) == len(label_id))
    token_ids = []

    for one_label, one_box in zip(label_id, bbox):
        assert(one_label>0 and one_label<len(labels))
        assert(len(one_box) == 4)
        token_ids.append(one_label+BINS_FOR_OBJ_LABEL_RANGE[0])
        token_ids.append(round(one_box[0]/image_size[0]*BINS_FOR_LOC_SIZE)+BINS_FOR_LOC_RANGE[0])
        token_ids.append(round(one_box[1]/image_size[1]*BINS_FOR_LOC_SIZE)+BINS_FOR_LOC_RANGE[0])
        token_ids.append(round((one_box[0]+one_box[2])/image_size[0]*BINS_FOR_LOC_SIZE)+BINS_FOR_LOC_RANGE[0])
        token_ids.append(round((one_box[1]+one_box[3])/image_size[1]*BINS_FOR_LOC_SIZE)+BINS_FOR_LOC_RANGE[0])
    
    return processor.tokenizer.decode(token_ids)

def find_assistant_content_sublist_indexes(l):
    '''
    A message from train_data/data.json may look like below:
        {
            "messages": [
                {'role': 'user', 'content': [{'type': 'image', 'image': 'train_data/1.jpeg'}, {'type': 'text', 'text': 'æè¿°ä¸€ä¸‹è¿™ä¸ªå›¾ç‰‡'}]}, 
                {'role': 'assistant', 'content': [{'type': 'text', 'text': 'è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä½å¹´è½»å¥³å­å’Œå¥¹çš„ç‹—åœ¨æµ·æ»©ä¸Šç©è€çš„åœºæ™¯ã€‚å¥³å­ç©¿ç€æ ¼å­è¡¬è¡«å’Œé»‘è‰²è£¤å­ï¼Œååœ¨æ²™æ»©ä¸Šï¼Œä¸å¥¹çš„é‡‘æ¯›çŠ¬äº’åŠ¨ã€‚å¥¹ä»¬çš„æ‰‹è‡‚ä¼¸å±•ç€ï¼Œä¼¼ä¹åœ¨è¿›è¡ŒæŸç§æ¸¸æˆæˆ–è®­ç»ƒã€‚èƒŒæ™¯æ˜¯å¹¿é˜”çš„æµ·æ´‹å’Œæ™´æœ—çš„å¤©ç©ºï¼Œé˜³å…‰æ´’åœ¨æ²™æ»©ä¸Šï¼Œè¥é€ å‡ºæ¸©æš–è€Œå®é™çš„æ°›å›´ã€‚æ•´ä½“ç”»é¢å……æ»¡äº†å¿«ä¹å’Œæ”¾æ¾çš„æ„Ÿè§‰ã€‚'}]}
            ]
        }
    After apply_chat_template, the text will look like below:
        ['<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>æè¿°ä¸€ä¸‹è¿™ä¸ªå›¾ç‰‡<|im_end|>\n<|im_start|>assistant\nè¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä½å¹´è½»å¥³å­å’Œå¥¹çš„ç‹—åœ¨æµ·æ»©ä¸Šç©è€çš„åœºæ™¯ã€‚å¥³å­ç©¿ç€æ ¼å­è¡¬è¡«å’Œé»‘è‰²è£¤å­ï¼Œååœ¨æ²™æ»©ä¸Šï¼Œä¸å¥¹çš„é‡‘æ¯›çŠ¬äº’åŠ¨ã€‚å¥¹ä»¬çš„æ‰‹è‡‚ä¼¸å±•ç€ï¼Œä¼¼ä¹åœ¨è¿›è¡ŒæŸç§æ¸¸æˆæˆ–è®­ç»ƒã€‚èƒŒæ™¯æ˜¯å¹¿é˜”çš„æµ·æ´‹å’Œæ™´æœ—çš„å¤©ç©ºï¼Œé˜³å…‰æ´’åœ¨æ²™æ»©ä¸Šï¼Œè¥é€ å‡ºæ¸©æš–è€Œå®é™çš„æ°›å›´ã€‚æ•´ä½“ç”»é¢å……æ»¡äº†å¿«ä¹å’Œæ”¾æ¾çš„æ„Ÿè§‰ã€‚<|im_end|>\n']

    This function tries to find the indexes of the assistant content in the input_ids list to build labels.
    '''
    # (Pdb++) processor.tokenizer.encode("<|im_start|>assistant\n")
    # [151644, 77091, 198]
    # (Pdb++) processor.tokenizer.encode("<|im_end|>\n")
    # [151645, 198]

    start_indexes = []
    end_indexes = []

    # Iterate through the list to find starting points
    for i in range(len(l) - 2):
        # Check if the current and next elements form the start sequence
        if l[i] == 151644 and l[i + 1] == 77091 and l[i+2] == 198:
            start_indexes.append(i+3)
            # Now look for the first 151645 and 198 after the start
            for j in range(i+3, len(l)-1):
                if l[j] == 151645 and l[j+1] == 198:
                    end_indexes.append(j+2) # **NOTE** the <|im_end|>\n 2 tokens should be included in the label, so that model can predicate end of output.
                    break  # Move to the next start after finding the end

    return list(zip(start_indexes, end_indexes))

def collate_fn(batch, processor, device):
    '''
    (Pdb++) processor.tokenizer.decode([i for i in range(*BINS_FOR_LOC_RANGE)])
    'ï®'â‡µâˆ‰âˆŠâˆ–âˆœâˆ¾â‰€â‰‹â‰Œâ‰“â‰œâ‰´â‰¿âŠŠâŠ‹âŠ”âŠ–âŠ£âŠ¦â‹â‹ªâ‹²âŒ¦âŒ§âºâˆâ¨â¬â³â¼â¾âŒâšâ«â¯âµâ’œâ’â’«â“„â“Šâ“™â“©â”‘â”™â”šâ”¥â•…â•‰â•â•â•â–šâ–¯â—ƒâ—šâ—¬â—´â˜ˆâ˜¤â˜¥â˜§â˜¬â™â™±âšƒâš„âš…âšâššâšâšŸâš±âš²âœ€âœŸâœ¢âµâŸ¡âŸ¦âŸ§âŸ³âŸ¾âŸ¿â ‡â¤„â¤ºâ¥‚â¥¹â§‰â§¼â§½â¨â¬Šâ¬Ÿâ­â®â®³â¯ˆâ¯‘â± â±±â²­â´¹âµ•â¸¾âº«â¼†â¼ â½Ÿâ½¼â¾›â¾§â¿ƒâ¿»ã‚•ã‚Ÿã„›ã„¡ã„¶ã„ºã…’ã…Ÿã†€ã‡»ãˆ‘ãˆ­ãˆ®ãˆ³ãˆ¹ã‰¥ã‰¦ã‰¹ã‰¿ãŠãŠ¨ã‹‘ã‹¥ã‹´ã‹ºã„ã•ã¯ã‚ãˆã“ã–ã±ã±ãŸï¿½ã¢¨ï¿½ã¨³ã«ªã«´ã¶³ãº¾ï¿½ä€€ï¿½ä‹ŒäŒ€ä€ä €ï¿½ä ¼ï¿½ä§ä¨°ä¨ºä´€ï¿½ä·…ä·¸ï¿½ê‚«ï¿½êŒ¼ï¿½ê²ê’µï¿½ê“½ê™­ê›ê¥ï¿½êŠê¦†ê¦‡ê¦Ÿê¦¨ê§ˆï¿½ê©Ÿêª‹êª‘êª•êª—êªœêª®êª±êª»êª¼ê«€ê«ê°ƒê°˜ê±œê²“ê²šê³™ê³¾ê´—ê´™êµ›ê¶ƒê¶•ê¶¨ê¸©ê¸¿ê¹„ê¹†ê¹‰ê¹“ê¹¢ê¹£ê¹¸êº³ê¿ê¿•ê¿§ë€©ë…ëƒµë„–ë„—ë„¢ë…‚ë†ë‡œëˆ‹ëˆšë‰ë‰¨ëŠšëŠ¡ë‹œë‹ªëŒ˜ëŒ¤ëŒ¸ëŸë¨ë„ëë´ë¸ë‘ë‘¿ë’¨ë“·ë”®ë”²ë•§ë–”ë–ªë˜­ëš€ëš ë›”ë›©ëœ…ë•ë°ëŸë ¡ë¡ë¡£ë¡µë£„ë£ë¤³ë¦ë¦ë¦³ë§„ë§†ë§ë§œë§«ë§»ë¨®ë©‚ë©­ëª´ë¬œë¬ ë¬«ë¬¾ë­¬ë®˜ë®¹ë¯•ë¯œë°¨ë°ªë±”ë²˜ë²›ë²±ë²´ë´½ëµ¤ëµ¨ë·—ë·˜ë¸“ë¸œë¹ªëºƒëº˜ëºµë»´ë¼ë¾”ì­ì‚ ì‚®ìƒìƒ™ì„ºì…¢ì†€ì†…ì†¤ì†¦ì†¬ì‡±ìˆµì‹¨ì‹´ìŒ°ìœì—ì˜ì¼ì‘‰ì‘ì‘»ì’”ì’¯ì“©ì•ì•–ì– ì–¾ì—ƒì——ì—œì—¨ì˜‚ì˜„ì˜ì˜¾ì˜¿ìœ§ìì–ì·ììì¨ìªì³ì ¡ì ´ì ¹ì¡€ì¡ªì¡µì¢ì¢¨ì£Œì£™ì£³ì¦‘ì§¥ì§´ì§¾ì¨“ì¨•ì©°ì©»ì©¼ìª—ì¬”ì¬˜ì®®ì¯•ì¯˜ì°ì°¯ì±ƒì±µì²§ì²®ì²¯ì³¬ì´‹ì´¢ìµ¥ì¶£ì¸ˆì¸™ìº¤ìº­ì»½ì¼™ì½¬ì¾€ì¿…ì¿½í€…í¦í‚…íƒ¶íƒ¹í„”í…£í†„í†§í†¹í‡¼í‰¤íŠ½í‹‚í‹‘íˆí™í¿í¶íí’œí“í“ªí“±í“·í“¼í”™í” í•ší•›í•í•Ÿí•§í•¶í–Ší–‹í–í–”í–˜í–¡í–¬í—£í—¿í˜–í˜­íš°í›í›½íŸí­í´íœï¤‰ï¤­ï¤²ï¤µï¤¼ï¥€ï¥‘ï¥’ï¥•ï¥˜ï¥™ï¥«ï¥¬ï¥°ï¥¿ï¦‹ï¦ï¦”ï¦–ï¦˜ï¦›ï¦ ï¦®ï¦¯ï¦ºï¦»ï¦¾ï§†ï§–ï§›ï§ï§Ÿï§§ï§³ï§ºï§½ï¨ƒï¨šï¨¢ï©Ÿï¬¤ï¬¬ï¬¼ï­’ï­•ï­›ï­ï­ï­Ÿï­¤ï­§ï­¨ï­®ï­°ï­±ï­·ï­¹ï­»ï®€ï®ƒï®„ï®…ï®ï®’ï®“ï®•ï®¦ï®®ï®°ï¯“ï¯œï¯©ï¯ªï¯¬ï¯­ï¯®ï¯·ï¯¹ï¯»ï¯¼ï°ƒï°Œï°ï°˜ï°™ï°œï°ï°¢ï°®ï°°ï°¼ï°¿ï±€ï±ï±ˆï±‹ï±ï±­ï²€ï²‡ï²ˆï²‹ï²ï²’ï²œï² ï²¬ï²»ï³‡ï³”ï³£ï³«ï´˜ï´°ï´½ï¿½ï¶°ï¸–ï¸´ï¸¹ï¹ï¹—ï¹¢ï¹¤ï¹©ï¹±ï¾°ï¿‚ï¿®ğŒ°ğŒ¹ğŒºğŒ½ğ‚ğƒğ„ï¿½ğ¹ğ¤‚ğ¤ğ¤ğ¤“ğ­‰ğ­ğ°‡ğ°°ï¿½ğ‘‚„ï¿½ğ‘˜ï¿½ğ’€¸ï¿½ğ’ºï¿½ğ’„·ï¿½ğ’Š‘ï¿½ğ’‹—ï¿½ğ’Œ¨ğ“ƒ¢ğ“ƒ°ï¿½ğ– šğ„ƒğ„…ğ„•ğ„™ğ„±ğ„´ğ„¹ğ…ğ…ªğ†£ğ†³ğ†¹ğ‡Šğ‡—ğ‡šğ‡œğ‡ ğ‰ğ–ğ˜ğ£ğ±ğ‘Šğ‘­ğ‘¼ğ‘½ğ’°ğ’·ğ’¿ğ“ğ“‹ğ“ğ“’ğ“˜ğ“¢ğ“¦ğ“«ğ“¿ğ”ğ”±ğ”´ğ”·ğ”¸ğ”½ğ•‚ğ•ƒğ•‹ğ•ğ•ğ•¥ğ•´ğ•ºğ–ğ–›ğ–ğ–ğ—©ğ—³ğ—½ğ˜Šğ˜‹ğ˜”ğ˜±ğ˜´ğ˜¿ğ™’ğ™ğ™Ÿğ™¬ğ™­ğ™»ğ™¾ğšˆğš‹ğš‘ğšŸğš ğš£ğ›½ğœ‚ğœ”ğœ™ï¿½ğŸ€„ğŸ„²ğŸ„¶ğŸ…ğŸ…–ğŸ…šğŸ…›ğŸ…¦ğŸ…¶ğŸ…»ğŸ…¼ğŸ†ƒğŸ††ğŸ†ğŸˆ¯ğŸˆ²ğŸˆ¹ğŸŒ‡ğŸŒ“ğŸ˜ğŸ‘ğŸ¿ğŸğŸ’ğŸ©ğŸ¯ğŸ€ğŸ‘ğŸ’¹ğŸ’ºğŸ“ŸğŸ“ªğŸ“¼ğŸ”€ğŸ”‚ğŸ”ƒğŸ”‡ğŸ”“ğŸ”¢ğŸ”¤ğŸ”©ğŸ•–ğŸ•šğŸ•œğŸ•ğŸ•ğŸ• ğŸ•¢ğŸ•³ğŸ–‡ğŸ–‘ğŸ–¶ğŸ—Ñ¨Úá¡Œá¸°áº€á¼®á½â„¬âš§â›¤ã³¬ê™‹ê¸‘ë”‰ë—ë¡‘ë¯‘ë»…ë¼ì„ì‰¡ì‹²ì±ì—¤ì©ì¿ìŸ™ì °ì¥‰íŠ­í•®ï®ğŸ…±ğŸ†’ğŸ•‹É˜Ê“Õƒà´´à½…á†ºáˆŠáˆ¨áˆ¾á‰áŒƒáŒ½á”­á ‚á ¬á¨¸á©‹á¶á¾”á¿á¿šâ™™âš‚âš—â¡¢â¤¦ë–°ë¤‚ë§ ë±‹ë±ì›¢ìœ¾ì³…ì»í»íƒ™í“–í“­í•±í›œï¤…ï¤†ï¦ƒï§©ï¨‚ğ¤”ğ­“ğ°¼ğ“ğ“°ğ™œğšğŸ…¢ğŸ‡È²Ê¶ÔˆÔ‘İ“İ¥à¤‘à¥±à¬‰à°³à°µà²Ÿá€á¼á‰¨áŠ’á‹©áŒ„áŒ”á§á’Œá”…á”Šá „á¨á¸ƒá¸»â”â˜µâš£â²¢ãˆªä¶µê²™ê²´ê³‚ë¡¼ì†Šì¼‡í‹í“¬í“®í“¶í“»ï¤¦ï¥ ï¥±ï­²ğ­Šğ±…ï¿½ğ–¥¨ğ‘³ğ“•ğ“¬ğ“¹ğ“¾ğ”“ğ•ğ•¡ğ•±ğ––ğ˜ğ˜ğ˜šğ™®ğ™°ğ™¸ğ™ºğ™¼ğ™½ğ™¿ğš„ğšğŸ……ğŸ…“Æˆà Œá™³ášŒá›…á›á¤Šá¸Šâ”½â•Šâ›‡â›âªâ«âŸ°ã„ã„“ã„§ã…–ã‰«ê¦”ï±Šàº‚á…£á¥”á¥¤â†¤â†·â‡â–¤â¶ãˆ¼ï¨·ğ“§â”²â€´â’Ÿâ’¡â°‚â°â°â°â°‘â°Ÿâ° â°¡â¼­ãŠ¥â’ â½ºã‡ºã‡½ï¨Šá•·â¨âºŸ'
    
    (Pdb++) processor.tokenizer.decode([i for i in range(*BINS_FOR_OBJ_LABEL_RANGE)])
    'á›‚á›™áá †á ¡á ¦á ®á ¯á ²á ·á¡á¡á¡¤á¡´á¡µá¤“á¥–á¥°á¨¦á¨§á¨¨á¨ªá¨¬á¨¯á¨³á¨µá©ƒá¬•á­£ï¿½á±šá² á´“á´¶áµ‚áµŒáµ¥áµ´á¶‡á¸ˆá¸ á¸§á¸´á¸¾á¹€á¹–á¹Ÿá¹ á¹«á¹±á¹·á¹¿áº„áºáº‘áº—á¼‰á¼“á¼­á½‹á½’á½ á½£á¾„á¾á¾‘á¾—á¾¦á¾§á¾¾á¿„á¿“á¿¡á¿¬âšâ‚Œâ„â„”â„£â„§â„¯â„°â„´â……â†œâ†«â†­â†±â†¹â†½â‡‡â‡œ'
    
    (Pdb++) pprint.pprint(batch[1])
    {
        'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x573 at 0x155302B82E00>,
        'image_id': 414738,
        'objects': {
                        'area': [632.9258500000003, 82680.72434999997, 12868.869650000002],
                        'bbox': [[265.05999755859375,
                                126.02999877929688,
                                33.86000061035156,
                                66.26000213623047],
                                [20.600000381469727,
                                1.0700000524520874,
                                270.4100036621094,
                                382.42999267578125],
                                [268.6099853515625,
                                69.66000366210938,
                                222.67999267578125,
                                88.9000015258789]],
                        'id': [341560, 425599, 1982651],
                        'iscrowd': [False, False, False],
                        'label': [90, 1, 81]
                    }
        }
    (Pdb++) texts[1]
    '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>æè¿°ä¸€ä¸‹è¿™ä¸ªå›¾ç‰‡ï¼ŒæŠŠå…¶ä¸­çš„ç‰©ä½“åç§°å’Œä½ç½®æ ‡æ³¨å‡ºæ¥<|im_end|>\n<|im_start|>assistant\n ..... <|im_end|>\n'
    '''

    messages = []
    for d in batch:
        if d['image'].size != (640, 480):
            continue
        messages.append(
            [
                {
                    'role': 'user', 
                    'content': [
                        {'type': 'image', 'image': d['image']}, 
                        {'type': 'text', 'text': 'æè¿°ä¸€ä¸‹è¿™ä¸ªå›¾ç‰‡ï¼ŒæŠŠå…¶ä¸­çš„ç‰©ä½“åç§°å’Œä½ç½®æ ‡æ³¨å‡ºæ¥'}
                    ]
                },
                {
                    'role': 'assistant', 
                    'content': [
                        {'type': 'text', 'text': convert_bbox_xyhw_and_label_id_to_tokens(d['objects']['bbox'], d['objects']['label'], processor, d['image'].size)}
                    ]
                }
            ]
        )

    # ** NOTE **: hack, we just keep (640, 480) images.
    if len(messages) == 0:
        return None, None

    texts = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=False) for msg in messages]
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=texts,
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to(device)

    input_ids_lists = inputs['input_ids'].tolist()
    assert len(messages) == len(input_ids_lists)

    labels_list = []
    for ids_list in input_ids_lists:
        label_ids = [-100] * len(ids_list) # -100 is the ignore index in loss function
        for begin_end_indexs in find_assistant_content_sublist_indexes(ids_list):
            label_ids[begin_end_indexs[0]:begin_end_indexs[1]] = ids_list[begin_end_indexs[0]:begin_end_indexs[1]]
        labels_list.append(label_ids)

    labels_ids = torch.tensor(labels_list, dtype=torch.int64)
    return inputs, labels_ids


def get_train_data_loader(processor, device, batch_size=None):
    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=partial(collate_fn, processor=processor, device=device))
    return train_loader

def get_val_data_loader(processor, device, batch_size=None):
    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=partial(collate_fn, processor=processor, device=device))
    return val_loader


